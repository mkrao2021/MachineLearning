# -*- coding: utf-8 -*-
"""GAN_illus.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bjyNoqGh1BvMTweWBWPbyYnqT028sKBR

## GAN loss

The GAN performs the following <font color=red>min-max</font> optimization
$$\max_{D}\min_{G}\Big(\log\left(D(x_{\rm real})\right) y_{\rm real} + \log\left(D(x_{\rm fake})\right) y_{\rm fake}\Big), $$

Note that the binary Cross Entropy loss between the target labels $x$ and the predicted labels $x$ are given by 

$${\rm BCE}(y,x) = -\Big(y \log(x) + (1-y) \log(1-x)\Big),$$

With the above definition of BCE loss, the GAN criterion can be rewritten as the <font color=red>max-min</font> optimization problem:

$$\min_{D}\max_{G}  \Big({\rm BCE}\left(D(x_{\rm real}),\underbrace{y_{\rm real}}_{1}\right) + {\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm fake}}_0\right)\Big)
$$

Note that the generator only maximizes the second term. 
Pytorch is designed to minimize functions. For this, we could consider the minimization of the negative of the second term
$$\min_{G}  ~-{\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm fake}}_0\right).
$$
Using the property of cross-entropy loss, one can simply swap the labels <font color=red>$y_{\rm fake} \rightarrow y_{\rm real}=1-y_{\rm fake}$</font> to rewrite the above expression as 
$$\min_{G}  ~{\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm real}}_1\right)
$$
Thus, the generator will try to get a label 1.  

### Thus, the GAN optimization alternates between the following steps. 

1. Discriminator optimization $\min_{D}  \Big({\rm BCE}\left(D(x_{\rm real}),\underbrace{y_{\rm real}}_{1}\right) + {\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm fake}}_0\right)\Big)
$

<font color=red>The discriminator optimization aims to assign $D(x_{\rm real}) \rightarrow 1$ and $D(x_{\rm fake}) \rightarrow 0$ </font>. This is a classification task and the neural network learns the discriminant. 

2. Generator optimization $\min_{G}  ~{\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm real}}_1\right)$

<font color=red>The generator tries to assign $D(x_{\rm fake}) \rightarrow 1$ </font>.

## Create data on a circle. You may change the model to see how the model will adapt to new points
"""

from torch.utils.data import DataLoader
from torch.utils.data import TensorDataset
import torch.nn as nn
import numpy as np
import torch
from torch.autograd import Variable
import torch.optim as optim
import matplotlib.pyplot as plt


# Defining the circle
N = 2000
x=np.linspace(-2*np.pi,2*np.pi,N)
y=np.sin(x)
z=np.cos(x)
Y = np.hstack((np.expand_dims(y,axis=1),np.expand_dims(z,axis=1)))

index = np.arange(N)
# Make the data look like a data set
X=np.expand_dims(x,axis=1)
index = np.expand_dims(index,axis=1)
# Use batch training
dataset=TensorDataset(torch.tensor(index,dtype=torch.long),torch.tensor(Y,dtype=torch.float))

plt.plot(y,z)

# The main structure of the generator, here is a simple linear structure

class generator(nn.Module):
    def __init__(self,input_dim=1, output_dim=1,base_size=20):
        super(generator, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.base_size = base_size
        self.net=nn.Sequential(
            nn.Linear(in_features=self.input_dim,out_features=self.base_size),
            nn.ReLU(),
            nn.Linear(self.base_size,10*self.base_size),
            nn.ReLU(),
            nn.Linear(10*self.base_size,10*self.base_size),
            nn.ReLU(),
            nn.Linear(10*self.base_size,self.output_dim)
            #nn.Linear(self.base_size,self.output_dim)
        )

    def forward(self, input:torch.FloatTensor):
        return self.net(input)

# The main structure of the discriminator, here is a simple linear structure

class discriminator(nn.Module):
    def __init__(self, input_dim=2, output_dim=1, base_size=10):
        super(discriminator, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.base_size = 64

        self.fc = nn.Sequential(
            nn.Linear(input_dim, self.base_size),
            nn.LeakyReLU(0.2),
            nn.Linear(self.base_size, 2*self.base_size),
            nn.LeakyReLU(0.2),
            nn.Linear(2*self.base_size, self.output_dim),
            nn.Sigmoid(),
        )

    def forward(self, x):
        x = self.fc(x)
        return x

"""## Function to display the discriminant along with fake and real points"""

def plotDiscriminant_And_Points(D,truePts,fakePts,Ngrid=20,ext=1.2,title=""):
# Construct testing meshgrid

  Ngrid = 20
  xtest = np.linspace(-ext, ext, Ngrid)
  ytest = np.linspace(-ext, ext, Ngrid)
  xtest, ytest = np.meshgrid(xtest, ytest)
  Xtest = np.hstack((xtest.ravel().reshape(-1,1),ytest.ravel().reshape(-1,1)))
  Xtest = torch.tensor(Xtest).type(torch.FloatTensor).cuda()
  yTest = D(Xtest)
  yTest = np.reshape(yTest.detach().cpu().numpy(),(Ngrid,Ngrid))
  s = plt.figure()
  s = plt.imshow(yTest, extent=[-ext,ext,-ext,ext], origin='lower')
  s = plt.colorbar()

  plt.scatter(truePts[:,0].cpu().numpy(),truePts[:,1].cpu().numpy(),c='m')
  plt.scatter(fakePts[:,0].detach().cpu().numpy(),fakePts[:,1].detach().cpu().numpy(),c='r')
  plt.title(title)
  plt.show()

"""## Initializing discriminator and generator"""

D = discriminator(base_size=30,input_dim=2, output_dim=1,).cuda()
G=generator(base_size=10,output_dim=2).cuda()

batch_sz = 200
z_dim = 1
dataloader=DataLoader(dataset,batch_size=batch_sz,shuffle=True)

G_optimizer = optim.Adam(generator.parameters(G), lr=0.0001)
D_optimizer = optim.Adam(discriminator.parameters(D), lr=0.001)
BCE_loss = nn.BCELoss().cuda()

index,x_ = next(iter(dataloader))
y_real_, y_fake_ = torch.ones(batch_sz, 1), torch.zeros(batch_sz, 1)
y_real_, y_fake_ = y_real_.cuda(), y_fake_.cuda()

"""## Test discriminator separately. 

We optimize the discriminator such that the cross-entropy loss between the data and the labels are minimized. This classification task would learn a discriminant, which classifies the two datasets

$$\min_{D}  \Big({\rm BCE}\left(D(x_{\rm real}),\underbrace{y_{\rm real}}_{1}\right) + {\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm fake}}_0\right)\Big)
$$
"""

for epoch in range(50):
    for batch_index,x_ in dataloader:
        z_ = 2*torch.rand((batch_sz, 1))-1

        x_, z_ = x_.cuda(), z_.cuda()

        D_optimizer.zero_grad()
        D_real = D(x_)
        D_real_loss = BCE_loss(D_real, y_real_)

        #G_ = z_
        G_ = G(z_)
        D_fake = D(G_)
        D_fake_loss = BCE_loss(D_fake, y_fake_)

        D_loss = D_real_loss + D_fake_loss
        D_loss.backward()
        D_optimizer.step()

        G_loss = BCE_loss(D_fake, y_real_)


    if(np.mod(epoch,10)==0):
        print("Dloss =",D_loss.detach().cpu().numpy(),";Gloss=",G_loss.detach().cpu().numpy())
        plotDiscriminant_And_Points(D,x_,G_,ext=1.2)

"""## Testing the generator independently.

The generator adjusts the parameters such that the cross entropy loss between the fake labels and the real labels is decreased. 

$$\min_{G}  ~{\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm real}}_1\right)$$

It moves the points to the high values of the discriminant. 
"""

for epoch in range(10):
    for batch_index,x_ in dataloader:
        z_ = 2*torch.rand((batch_sz, 1))-1
        x_, z_ = x_.cuda(), z_.cuda()

        G_ = G(z_)
        D_fake = D(G_)
        G_loss = BCE_loss(D_fake, y_real_)
        
        G_loss.backward()
        G_optimizer.step()

    if(np.mod(epoch,5)==0):
        print("Dloss =",D_loss.detach().cpu().numpy(),";Gloss=",G_loss.detach().cpu().numpy())
        plotDiscriminant_And_Points(D,x_,G_)

"""## Joint optimization

Note from above that if the discriminator is not updated, the generator will converge to points that does not make sense. We now perform the min max optimization.

"""

Ninner = 1
train_hist = {}
train_hist['D_loss'] = []
train_hist['G_loss'] = []


for epoch in range(2000):
    for batch_index,x_ in dataloader:
        z_ = 2*torch.rand((batch_sz, 1))-1
        x_, z_ = x_.cuda(), z_.cuda()

        # update D network
        
        for i in range(Ninner):
          D_optimizer.zero_grad()
          D_real = D(x_)
          D_real_loss = BCE_loss(D_real, y_real_)

          G_ = G(z_)
          D_fake = D(G_)
          D_fake_loss = BCE_loss(D_fake, y_fake_)

          D_loss = D_real_loss + D_fake_loss
          D_loss.backward()
          D_optimizer.step()

        # update G network
        for i in range(Ninner):
          G_optimizer.zero_grad()
          G_ = G(z_)
          D_fake = D(G_)
          G_loss = BCE_loss(D_fake, y_real_)
        
          G_loss.backward()
          G_optimizer.step()

    train_hist['D_loss'].append(D_loss.item())
    train_hist['G_loss'].append(G_loss.item())

    if(np.mod(epoch,50)==0):
        print("Dloss =",D_loss.detach().cpu().numpy(),";Gloss=",G_loss.detach().cpu().numpy())
        z_ = z_.cuda()
        plotDiscriminant_And_Points(D,x_,G_)

"""# Losses used for GAN optimization

1. <font color=blue>Discriminator optimization $\min_{D}  \Big({\rm BCE}\left(D(x_{\rm real}),\underbrace{y_{\rm real}}_{1}\right) + {\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm fake}}_0\right)\Big)
$ </font>

The discriminator loss will achieve a low value when it can reliably separate the two classes.

2. <font color=red>Generator optimization $\min_{G}  ~{\rm BCE}\left(D(\underbrace{x_{\rm fake}}_{G[z]}),\underbrace{y_{\rm real}}_1\right)$</font>.

The generator loss will achieve a low value when it can successfully fool the discriminator


"""

plt.figure(figsize=(16,8))
s=plt.plot(train_hist['D_loss'],c='b')
s=plt.plot(train_hist['G_loss'],c='r')
s = plt.ylim((0,3))
s = plt.grid()
s=plt.legend(('Discriminator loss','Generator loss'))