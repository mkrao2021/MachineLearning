# -*- coding: utf-8 -*-
"""kernel_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JJbt-SbR0lr2Xbz7RMW-rozdrf805bdd
"""

import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter

plt.rcParams.update({'font.size': 20})
import numpy as np
import scipy.special

"""# Kernel regression
 
 We are fitting a polynomial $$y= a_o + a_1 ~\varphi_{k,1}(x) + .. a_d ~\varphi_{d,d}(x)$$
 
We will consider the weighted polynomials $$ \varphi_{k,d} = \sqrt{{d\choose k}} ~x^k $$ to obtain a simple expression for the kernel. Note that we are applying a lifting of the original feature $x$ to obtain
$$x \rightarrow \begin{bmatrix}1\\\varphi_{1,d}(x)\\\vdots \\ \varphi_{d,d}(x)\end{bmatrix} = \phi(x)$$

The polynomial matrix for a set of points $x_1,..x_N$ evaluated below



"""

def polymatrix(x,d):
    x = np.array(x)
    N = x.size
    x = np.reshape(x,(N,1))
    X = np.ones(x.shape)
    for i in range(1,d+1):
        weight = np.sqrt(scipy.special.binom(d, i))
        X = np.hstack((X,weight*np.power(x,i)))    
    return(X)

"""### Testing the polymatrix

"""

print(polymatrix(1,3))

"""# Kernel matrix
 
With the above normalized polynomials, the innerproduct in feature space is specified by 
$$\left\langle \phi(x),\phi(y) \right\rangle = (1+xy)^d $$

The kernel matrix $\mathbf K$ is obtained from the training data as $$(\mathbf K)_{i,j} = \left\langle \phi(x_i),\phi(x_j) \right\rangle = (1+x_i x_j)^d$$ 
"""

def polykernel(x,y,d):
    return (np.power(1+(x*y),d))

def polykernelmatrix(x,y,d):
    Nx = x.size
    Ny = y.size
    K = np.zeros((Nx,Ny))
    for i in range(Nx):
        for j in range(Ny):
            K[i,j] = polykernel(x[i],y[j],d)
    return(K)

"""## Testing the polynomial kernel matrix"""

d = 4 # degree 1 polynomial
npoints = 8
xtraining = np.arange(npoints)

X = polymatrix(xtraining,d)
K = polykernelmatrix(xtraining,xtraining,d)

s=plt.figure()
s=plt.imshow(K)
s=plt.figure()
s=plt.imshow(X@X.T)

"""## Setting some test data"""

npoints = 8

# Noise variance
sigma = 0.2

xtraining = np.arange(npoints)
yorig = np.sin(xtraining) 
ytraining = yorig + sigma*np.random.normal(size=npoints)

fig = plt.figure()
ax = fig.gca()
cs = ax.plot(xtraining, ytraining,'ro',label='Training samples')
cs = ax.plot(xtraining, yorig,'b')
cs = ax.plot(xtraining,yorig,'ko',label='Original samples')
legend = ax.legend(loc='upper left', shadow=True, fontsize='x-small')
c=plt.title('Data')

"""## Regular polynomial ridge regression

Ridge regression
$$\theta = (\mathbf X^T\mathbf X + \lambda \mathbf I)^{-1}\mathbf X^T \mathbf y$$
"""

N = xtraining.size
d = 4
X = polymatrix(xtraining,d)
lam = 0.1

theta = np.linalg.inv(X.T@X + lam*np.eye(d+1))@X.T@ytraining
print('Coefficients',theta)

xtest = np.arange(0,7,0.1)
Xtest = polymatrix(xtest,d)

ytest = Xtest@(theta)
fig = plt.figure()
plt.plot(xtraining, ytraining,'ro',yorig)
plt.plot(xtest, ytest,'ro',yorig)
plt.ylabel('y')
plt.xlabel('x')
plt.show()

"""## Kernel regression using dual formulation

$$\mathbf w = (\mathbf K + \lambda \mathbf I)^{-1}\mathbf y$$

$$\boldsymbol\theta = \mathbf X^T \mathbf w$$
"""

K = polykernelmatrix(xtraining,xtraining,d)
N = xtraining.size

w = np.linalg.inv(K+lam*np.eye(N))@ytraining
print(w)
theta = (polymatrix(xtraining,d).T)@w
print(theta)

"""## Regression using dual coefficients


"""

xnew = np.arange(1.8,2)
kvector = polykernelmatrix(xtraining,xnew,d)
prediction = w@kvector
print('Comparison with training data',kvector)

print('prediction=',prediction)

xtest = np.arange(0,7,0.1)
kvector = polykernelmatrix(xtraining,xtest,d)
ytest = w@kvector

s=fig = plt.figure()
s=plt.plot(xtraining, ytraining,'ro',yorig)
s=plt.plot(xtest, ytest)
s=plt.ylabel('y')
s=plt.xlabel('x')