# -*- coding: utf-8 -*-
"""manifold_illustration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IOnedTdsP4lZ8FPkBgW35S_GV0pXaG2g
"""

import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter

plt.rcParams.update({'font.size': 20})
plt.rcParams['figure.figsize'] = [12, 6]

import numpy as np
import scipy.special

"""# Benefits of manifold assumption: illustration
 
Consider the quadratic polynomial function in five variables 
$$f(x,y) = {\color{red}a} + {\color{red}b}~x + {\color{red}c}~y + {\color{red}d} ~x^2 + {\color{red}e} ~y^2 + {\color{red}f}~ xy $$

Assume that the data is constrained to a line $\color{blue} {y=x}$. If we are only interested in points on the line, we can reduce the above quadratic polynomial as 
$$f(x,y) = {\color{red}a} + \color{red}{(b+c)}~{\color{blue} x} + \color{red}{(c+d+e)}~{\color{blue} x^2} $$

Note that the number of unknowns reduced from 5 to 3, implying a proportionate reduction in the number of training data points needed to learn the function. The two functions will be very different for points not on the line  $\color{blue} {y=x}$


"""

def TwoDfunction(x,y,coeffs):
  f = coeffs[0] + coeffs[1]*x + coeffs[2]*y + coeffs[3]*x**2 + coeffs[4]*y**2 + coeffs[5]*x*y
  return(f)

def OneDfunction(x,y,coeffs):
  f = coeffs[0] + coeffs[1]*x + coeffs[2]*x**2
  return(f)

"""Plotting the two functions on the straightline $\color{blue} {y=x}$"""

x = np.arange(-2,2,0.1)
y = np.arange(-2,2,0.1)

# coefficients of the 2D function
coeffs = [1,2,-3,1,-2,-1]

# coefficients of the equivalent 1D function
coeffs_reduced = np.zeros(3)
coeffs_reduced[0] = coeffs[0]
coeffs_reduced[1] = coeffs[1]+coeffs[2]
coeffs_reduced[2] = coeffs[3]+coeffs[4]+coeffs[5]

# plotting the two functions on the line
f1 = TwoDfunction(x, x,coeffs)
f2 = OneDfunction(x, x,coeffs_reduced)
s = plt.figure()
s=plt.plot(x,f1,'r')
s=plt.plot(x,f2,'b:')
s = plt.legend(('2D function','1D function'))

"""Plotting the two functions for all $x$ and $y$ values. Note that the two functions are drastically different away from the line. However, for all data points of interest, the 1D function is a good enough model"""

function = TwoDfunction(x[:,None], y[None,:],coeffs)
s=plt.imshow(function,extent=[-2,2,-2,2],vmin=-3,vmax=3,origin='lower')
s=plt.plot(x,y,'r')
s = plt.title('2D function')

function = OneDfunction(x[:,None], y[None,:],coeffs_reduced)
s = plt.figure()
s=plt.imshow(function,extent=[-2,2,-2,2],vmin=-3,vmax=3,origin='lower')
s=plt.plot(x,y,'r')
s = plt.title('1D function')

"""###Comparing support vector regression and kernel regression

We will compare SVR and kernel reggresion to study the impact. The main difference is the tube constraint, whose width is specified by epsilon. As tube becomes bigger, one would obtain a more approximate fit. With increasing epsilon, one would need fewer support points, which can reduce the complexity of the kernel fit. 
"""

# Training data on the line

xtrain = 2*np.random.randn(30)
ytrain = xtrain
ftrain = TwoDfunction(xtrain, ytrain,coeffs)

xtrain = np.array(xtrain).reshape(-1,1)
ytrain = np.array(ytrain).reshape(-1,1)
X = np.hstack((xtrain,ytrain))
from sklearn.svm import SVR 
from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import GridSearchCV

svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1,epsilon=5),
                   param_grid={"C": [1e0, 1e1, 1e2, 1e3],
                               "gamma": np.logspace(-2, 2, 5)})

kr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1),
                  param_grid={"alpha": [1e0, 0.1, 1e-2, 1e-3],
                              "gamma": np.logspace(-2, 2, 5)})

svr.fit(X, ftrain)
kr.fit(X, ftrain)


svrfit = svr.predict(X)
krfit = kr.predict(X)


s=plt.figure
s=plt.scatter(xtrain,ftrain,c='g',s=150, label='Original',marker="o")
s=plt.scatter(xtrain,svrfit,c='r',s=50, label='SVR',marker="*")
s=plt.scatter(xtrain,krfit,c='k',s=50, label='KR',marker="P")


s=plt.legend(('Original','SVR fit','Ridge fit'),loc='best')


s=plt.figure()
s=plt.plot(xtrain,svrfit,'r*')

sv_ind = svr.best_estimator_.support_
plt.scatter(xtrain[sv_ind], ftrain[sv_ind], c='m', s=150, label='SVR support vectors',
            zorder=2, edgecolors=(0, 0, 0))

s=plt.legend(('SVR fit','Support vectors'),loc='best')