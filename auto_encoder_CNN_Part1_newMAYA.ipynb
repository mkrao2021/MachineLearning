{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "auto_encoder_CNN_Part1_newMAYA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSE4csYrUBp7"
      },
      "source": [
        "# Auto encoder for MNIST data\n",
        "An autoencoder is a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the input.\n",
        "\n",
        "In this homework, we are going to explore some insight about the auto encoder such as how the latent space in the middle of the network looks like, how the image interpolates between two number labels, etc. Moreover, we will try to improve our existing auto encoder by defining it with the convolutional neural network (CNN), trying to add some noise and trying to add regularization directly to the loss function.\n",
        "\n",
        "### Main tasks for the homework\n",
        "\n",
        "## Part 1\n",
        "\n",
        "1. First, please go over the linear autoenccoder code on ICON auto_encoder_FCN.ipynb to see how the implemented linear auto encoder works. Linear auto encoder has limited representation power, especially when two latent variables are used. This results in blurred reconstructions and inability to fully capture some digits. \n",
        "\n",
        "2. Use more latent variables, layers, and features/layer in the linear auto-encoder to improve the representation power. You can use the MSE of the recovered images (compared to the originals in the MNIST test data) as a measure of representation power. The goal is to come up with minimal testing. Please keep track of the training and testing error for each setting. You can report these for each setting you have tried in a table within the comments.  Demonstrate that as you increase the representation power (e.g increase in latent variables) or features/layer, the MSE of the autoencoder on training data will decrease, while that on the test data may saturate or go up. \n",
        "\n",
        "3. Try the following regularization strategies to overcome the above problem (a) Explicit l2 regularization (b) Explicit l1 regularization (c) noise augmentation of inputs (d) early stopping. \n",
        "\n",
        "Here are some useful links for reference:\n",
        "\n",
        "https://debuggercafe.com/adding-noise-to-image-data-for-deep-learning-data-augmentation/\n",
        "\n",
        "https://scikit-image.org/docs/dev/api/skimage.util.html#skimage.util.random_noise\n",
        "\n",
        "https://discuss.pytorch.org/t/how-to-add-noise-to-mnist-dataset-when-using-pytorch/59745 (optional)\n",
        "\n",
        "We then try to add regularization directly to the loss function.\n",
        "\n",
        "Here are some useful links for reference:\n",
        "\n",
        "https://debuggercafe.com/sparse-autoencoders-using-l1-regularization-with-pytorch/\n",
        "\n",
        "https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
        "\n",
        "https://stackoverflow.com/questions/44641976/in-pytorch-how-to-add-l1-regularizer-to-activations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6u7DP-jDxaz"
      },
      "source": [
        "## Load the data and setup the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGyPZHKimdWZ"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from skimage.util import random_noise "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZJ4YRAmgFa"
      },
      "source": [
        "# Download the MNIST datasets from Prof. Jacob's google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejipXNWEmIqh",
        "outputId": "d8223eb0-6d0d-40dc-a2df-433586be5cdd"
      },
      "source": [
        "# Run the followin only once to download the files. Note where the data is copied to.\n",
        "!gdown --id '1fAW-pvhBWiXxE-H-WEOurS1Ta4V5mYe0'\n",
        "!gdown --id '1zSVOn9lJa4eF-jubtZwdtO-t0UScnJk0'"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fAW-pvhBWiXxE-H-WEOurS1Ta4V5mYe0\n",
            "To: /content/mnist_train.pickle\n",
            "47.5MB [00:00, 180MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zSVOn9lJa4eF-jubtZwdtO-t0UScnJk0\n",
            "To: /content/mnist_test.pickle\n",
            "7.92MB [00:00, 125MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbJTB5AkmmfS"
      },
      "source": [
        "# Once copied, load them to python\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIT8Gh7Tma94"
      },
      "source": [
        "import pickle\n",
        "with open('/content/mnist_train.pickle', 'rb') as data:\n",
        "    mnist_train = pickle.load(data)\n",
        "with open('/content/mnist_train.pickle', 'rb') as data:\n",
        "    mnist_test = pickle.load(data)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUv0q22pb74z"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [12, 6]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqfM2JwZLXze"
      },
      "source": [
        "## Setting up the fully connected autoencoder class.\n",
        "\n",
        "You may try with different number of features in the bottle neck layer. More parameters can improve the representation, but may result in poor generalization. The visualization is only posssible when the number of features in the bottleneck layer is 2.\n",
        "\n",
        "The encoder uses the following structure\n",
        "\n",
        " <font color=\"red\">784 $\\rightarrow$ 128$\\rightarrow$ 32 $\\rightarrow$ 12 $\\rightarrow$ 2</font>\n",
        "\n",
        "The decoder uses the following structure\n",
        "\n",
        " <font color=\"red\">2 $\\rightarrow$ 12$\\rightarrow$ 32 $\\rightarrow$ 128 $\\rightarrow$ 784</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbbrNmEnLXV5"
      },
      "source": [
        "class LinearAutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearAutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(True), \n",
        "            nn.Linear(32, 12), \n",
        "            nn.ReLU(True), \n",
        "            nn.Linear(12, 2),\n",
        "            nn.Tanh()\n",
        "            )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(2, 12),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(12, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 128),\n",
        "            nn.ReLU(True), \n",
        "            nn.Linear(128, 28 * 28), \n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nt_VjOLsFk"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "Define the sparse loss function as an example. You may also adapt this function for your needs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w93d1VgFDxa2"
      },
      "source": [
        "# define the l1/l2 loss on the weights. normtype = 2 for l2 regularization, =1 for l1 regularization\n",
        "\n",
        "def weightloss(model,normtype):\n",
        "    loss = 0\n",
        "    for W in model.parameters():\n",
        "      l2_reg = W.norm(normtype)\n",
        "    return loss\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dX3gH0nL-_v"
      },
      "source": [
        "## TODO: modify the following for Part 1.\n",
        "\n",
        "See list of todos above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlF_ZX_UcKUP"
      },
      "source": [
        "def train(model, num_epochs=5, batch_size=2048, learning_rate=1e-3):\n",
        "    torch.manual_seed(42)\n",
        "    criterion = nn.MSELoss() # mean square error loss\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=1e-4)    # Here the weight_decay parameter is adding L2 regularization\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(mnist_training, \n",
        "                                               batch_size=batch_size, \n",
        "                                               shuffle=True)\n",
        "    \n",
        "    models = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for data in train_loader:\n",
        "            img, _ = data\n",
        "\n",
        "            # (TODO) Add some noise augmentation (This is just one way to add the noise, feel free to come up with other ways)\n",
        "            img = torch.tensor(random_noise(img,mode='gaussian',mean=0,var=0.05,clip=True))\n",
        "            img=torch.tensor(random_noise(img,mode='s&p',salt_vs_pepper=0.5,clip=True))\n",
        "            img = torch.tensor(random_noise(img,mode='speckle',mean=0,var=0.05,clip=True))\n",
        "\n",
        "            img = Variable(img).cuda(device).type(torch.cuda.FloatTensor)\n",
        "            img = img.view(img.size(0), -1)\n",
        "            recon = model(img)\n",
        "            loss = criterion(recon, img)\n",
        "\n",
        "            # (TODO) Add L1/L2 regularization HERE\n",
        "            m_loss = criterion(recon,img)\n",
        "            loss = m_loss + 0.01 * weightloss(model,1)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "     \n",
        "        # Saving the models at each epoch for visualization purposes\n",
        "        training_loss.append(loss)\n",
        "        fname = 'dict'+str(epoch)\n",
        "        torch.save(model.state_dict(), fname)\n",
        "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
        "\n",
        "        # (TODO) Add early stopping HERE\n",
        "        if (epoch > 2) and (validation_loss[epoch]-validation_loss[epoch-1] < 0.01):\n",
        "          print('STOP')\n",
        "          break\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoSkNbbsY94L"
      },
      "source": [
        "## Training block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "2vyz5k4_cQi3",
        "outputId": "b08c1153-b4b1-4600-d546-2163f038826f"
      },
      "source": [
        "model = LinearAutoEncoder().cuda(device) \n",
        "\n",
        "max_epochs = 40\n",
        "model = train(model, num_epochs=max_epochs,batch_size=100,learning_rate=1e-2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e127ddbce631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-3bdc31f89d34>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m         model.parameters(), lr=learning_rate, weight_decay=1e-4)    # Here the weight_decay parameter is adding L2 regularization\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     train_loader = torch.utils.data.DataLoader(mnist_training, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                shuffle=True)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mnist_training' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpyTHURvNVzm"
      },
      "source": [
        "## Evaluate loss on test data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "4CSNaGLfNhpY",
        "outputId": "2a76b434-33a1-455b-e2bd-c42df96e877f"
      },
      "source": [
        "criterion = nn.MSELoss() \n",
        "batch_size = 1000\n",
        "train_loader = torch.utils.data.DataLoader(mnist_training, \n",
        "                                               batch_size=batch_size, \n",
        "                                               shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testing, \n",
        "                                               batch_size=batch_size, \n",
        "                                               shuffle=False)\n",
        "\n",
        "criterion = nn.MSELoss(reduction='mean') # mean square error loss\n",
        "\n",
        "trainingloss = 0\n",
        "for data in train_loader:\n",
        "    img, _ = data\n",
        "    img = Variable(img).cuda(device).type(torch.cuda.FloatTensor)\n",
        "    img = img.view(img.size(0), -1)\n",
        "    recon = model(img)\n",
        "    trainingloss += criterion(recon, img)\n",
        "trainingloss = trainingloss.detach().cpu().numpy()/train_loader.batch_size\n",
        "\n",
        "testloss = 0\n",
        "for data in test_loader:\n",
        "    img, _ = data\n",
        "    img = Variable(img).cuda(device).type(torch.cuda.FloatTensor)\n",
        "    img = img.view(img.size(0), -1)\n",
        "    recon = model(img)\n",
        "    testloss += criterion(recon, img)\n",
        "\n",
        "testloss = testloss.detach().cpu().numpy()/test_loader.batch_size\n",
        "print(\"Training loss=\",trainingloss,\"Testing loss=\",testloss)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-c6e224a7362d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m train_loader = torch.utils.data.DataLoader(mnist_training, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                shuffle=False)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mnist_training' is not defined"
          ]
        }
      ]
    }
  ]
}